# CLAUDE.md ‚Äî DINO-VO Reimplementation Project

## Project Overview

**Objective:** Reimplement DINO-VO from the paper "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model" [Azhari & Shim, IEEE RA-L, July 2025].

**Paper Reference:** arXiv:2507.13145v1

**Dataset:** EuRoC MAV Dataset (starting with single sequence, e.g., MH01)

**Hardware:** RTX 5060 Ti (16GB VRAM), Intel i5-14400F

---

## CRITICAL RULES ‚Äî READ BEFORE ANY ACTION

### Rule 1: NO FAKE IMPLEMENTATIONS
- **NEVER** create mock data, synthetic placeholders, or dummy implementations
- **NEVER** generate fake sensor readings, fake images, or fake ground truth
- **NEVER** use `random.rand()` or similar to simulate real data
- If data is missing, **STOP and ask the user** how to proceed
- If a component cannot be implemented, **STOP and explain why**
- Every piece of data must come from the actual EuRoC dataset

### Rule 2: EXPLAIN BEFORE CODING
- Before writing ANY code, explain:
  1. **WHAT** we are implementing (component name, purpose)
  2. **WHY** this component exists (its role in the pipeline)
  3. **HOW** it connects to other components
  4. **REFERENCE** to the specific section/equation in the paper
- Wait for user acknowledgment before proceeding with implementation

### Rule 3: INCREMENTAL DEVELOPMENT
- Implement ONE component at a time
- Test each component before moving to the next
- Never implement multiple modules in a single step
- If a test fails, fix it before proceeding

### Rule 4: GPU UTILIZATION
- All tensor operations must be GPU-compatible
- Use `torch.cuda.is_available()` checks
- Implement proper device management (`.to(device)`)
- Monitor VRAM usage ‚Äî stay under 14GB to leave headroom

### Rule 5: NO DOCKER
- All setup is local, no containerization
- Use conda/pip for dependencies
- Document all system requirements clearly

---

## Project Structure

```
dino-vo/
‚îú‚îÄ‚îÄ CLAUDE.md                 # This file
‚îú‚îÄ‚îÄ README.md                 # Project documentation
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ setup.py                  # Package setup (optional)
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ default.yaml          # Training/inference configuration
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ euroc/                # EuRoC dataset (downloaded, not committed)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MH_01_easy/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ mav0/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ cam0/     # Left camera images
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ cam1/     # Right camera images (not used for mono)
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ imu0/     # IMU data (not used in DINO-VO)
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ state_groundtruth_estimate0/  # Ground truth poses
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ README.md             # Dataset download instructions
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dino_vo.py        # Main DINO-VO model class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keypoint_detector.py    # Salient Keypoint Detector (Sec III-A)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_descriptor.py   # DINOv2 + FinerCNN (Sec III-B)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finer_cnn.py      # Lightweight CNN encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_matching.py     # Transformer-based matching (Sec III-C)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pose_estimation.py      # Differentiable pose layer (Sec III-D)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ euroc.py          # EuRoC dataset loader
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transforms.py     # Image preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geometry.py       # Geometric utilities (Essential matrix, SVD, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualization.py  # Trajectory plotting, match visualization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py        # ATE computation
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ losses/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ losses.py         # Matching loss (Eq. 12) and Pose loss (Eq. 13)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_euroc.py     # Dataset download script
‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Evaluation script
‚îÇ   ‚îî‚îÄ‚îÄ visualize.py          # Visualization script
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks for exploration (optional)
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/              # Saved model weights
‚îÇ
‚îú‚îÄ‚îÄ outputs/                  # Evaluation outputs (trajectories, plots)
‚îÇ
‚îî‚îÄ‚îÄ tests/                    # Unit tests for components
    ‚îú‚îÄ‚îÄ test_detector.py
    ‚îú‚îÄ‚îÄ test_descriptor.py
    ‚îú‚îÄ‚îÄ test_matching.py
    ‚îî‚îÄ‚îÄ test_pose.py
```

---

## Implementation Order

Follow this order strictly. Do not skip steps.

### Phase 1: Environment Setup
1. Create project directory structure
2. Create `requirements.txt` with dependencies
3. Verify GPU availability
4. Download EuRoC MH01 sequence

### Phase 2: Data Pipeline
5. Implement EuRoC dataset loader (images + ground truth)
6. Implement image preprocessing (resize to 476√ó742 as per paper)
7. Test: Load and visualize a single image pair

### Phase 3: Keypoint Detector (Paper Section III-A)
8. Implement Gaussian filter + Sobel gradient computation
9. Implement grid-based MaxPooling (kernel=14, stride=14)
10. Implement Non-Maximum Suppression (NMS) with radius=8
11. Implement gradient thresholding + top-k selection
12. Test: Visualize detected keypoints on sample image

### Phase 4: Feature Descriptor (Paper Section III-B)
13. Load pretrained DINOv2-ViT-S (frozen, from torch hub)
14. Implement FinerCNN encoder (basic layers from XFeat architecture)
15. Implement feature fusion (concatenation + linear projection)
16. Test: Extract features for detected keypoints

### Phase 5: Feature Matching (Paper Section III-C)
17. Implement transformer-based matching layer (based on LightGlue)
18. Implement self-attention with rotary positional encoding
19. Implement cross-attention between image pairs
20. Implement soft assignment matrix computation (Eq. 5-8)
21. Implement confidence prediction MLP (Eq. 9)
22. Test: Match keypoints between consecutive frames

### Phase 6: Pose Estimation (Paper Section III-D)
23. Implement weighted 8-point algorithm
24. Implement Essential matrix decomposition
25. Implement cheirality check for pose selection
26. Test: Estimate relative pose from matches

### Phase 7: Loss Functions (Paper Section III-E)
27. Implement matching loss (Eq. 12)
28. Implement pose loss (Eq. 13)
29. Implement combined objective (Eq. 14)

### Phase 8: Training Pipeline
30. Implement training loop with proper logging
31. Implement keyframe selection logic (24px threshold)
32. Implement learning rate scheduling
33. Test: Run single training iteration

### Phase 9: Evaluation
34. Implement trajectory accumulation
35. Implement ATE metric computation
36. Run evaluation on EuRoC MH01
37. Compare with paper's reported results

### Phase 10: Documentation
38. Write comprehensive README.md
39. Document all functions and classes
40. Create usage examples

---

## Key Implementation Details from Paper

### Salient Keypoint Detector (Section III-A)
- GaussianFilter: kernel_size=5, std=2.0
- MaxPooling: kernel_size=14 (matches DINOv2 patch size), stride=14
- NMS radius: rNMS=8
- Gradient threshold: 0.01
- Top-k keypoints: 512

### Feature Descriptor (Section III-B)
- DINOv2-ViT-S encoder: output shape H/14 √ó W/14 √ó 384
- FinerCNN: outputs H √ó W √ó 64
- Final descriptor dimension: 192 (after Linear projection)

### Feature Matching (Section III-C)
- Transformer layers: L=12
- Attention heads: 3
- Head dimension: 64
- Uses rotary positional encoding

### Pose Estimation (Section III-D)
- Weighted 8-point algorithm
- SVD for Essential matrix computation
- Cheirality check for pose disambiguation

### Training Details (Section IV-A)
- Image resolution for EuRoC: 476√ó742
- First 4 epochs: matching loss only
- Next 10 epochs: combined loss
- Œªr=180, Œªt=400
- Œªp: 0.0 ‚Üí 0.9 (increment 1.5e-4 per step from epoch 5)

### Keyframe Selection (Section III-F)
- Keyframe when mean pixel displacement > 24px
- For EuRoC: process alternate frames

---

## Dependencies

```
# Core
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
opencv-python>=4.8.0
scipy>=1.10.0

# DINOv2
# Loaded via torch.hub from facebookresearch/dinov2

# Configuration
pyyaml>=6.0
omegaconf>=2.3.0

# Visualization
matplotlib>=3.7.0
tqdm>=4.65.0

# Evaluation
evo>=1.20.0  # For trajectory evaluation (optional, can implement manually)

# Development
jupyter>=1.0.0
pytest>=7.3.0
```

---

## GPU Memory Considerations

**RTX 5060 Ti (16GB VRAM) Budget:**

| Component | Estimated VRAM |
|-----------|----------------|
| DINOv2-ViT-S (frozen) | ~350MB |
| FinerCNN | ~50MB |
| Feature Matching (12 layers) | ~200MB |
| Image pair (476√ó742) | ~10MB |
| Intermediate activations | ~2-4GB |
| **Total (training)** | ~4-6GB |
| **Headroom** | ~10GB |

**Optimization if needed:**
- Use FP16 (automatic mixed precision)
- Gradient checkpointing for transformer layers
- Reduce batch size (paper doesn't specify, likely 1-4)

---

## Checkpoints and Verification

After each component, verify:

1. **Keypoint Detector:**
   - Outputs 512 keypoints per image
   - Keypoints are distributed across image (not clustered)
   - Keypoints align to 14√ó14 grid

2. **Feature Descriptor:**
   - DINOv2 features: shape (K, 384)
   - FinerCNN features: shape (K, 64)
   - Combined features: shape (K, 192)

3. **Feature Matching:**
   - Assignment matrix P: shape (K, K)
   - Correspondences: list of (i, j) pairs
   - Confidences: values in [0, 1]

4. **Pose Estimation:**
   - Rotation R: valid SO(3) matrix
   - Translation t: unit vector (up-to-scale)
   - Passes cheirality check

---

## Communication Protocol

When implementing each component:

1. **Claude says:** "I'm about to implement [COMPONENT NAME]. This is [DESCRIPTION]. In the paper, this is described in [SECTION]. It connects to [OTHER COMPONENTS] because [REASON]. Should I proceed?"

2. **User reviews and approves**

3. **Claude implements with comments referencing paper equations**

4. **Claude proposes a test:** "To verify this works, I'll [TEST DESCRIPTION]. This should output [EXPECTED RESULT]."

5. **User runs test and confirms**

6. **Move to next component**

---

## What NOT To Do

- ‚ùå Generate synthetic images or fake data
- ‚ùå Use random numbers to simulate features
- ‚ùå Skip components or implement stubs
- ‚ùå Combine multiple implementation steps
- ‚ùå Proceed without user confirmation
- ‚ùå Ignore paper specifications (use exact hyperparameters)
- ‚ùå Use Docker or containerization
- ‚ùå Download models without user consent
- ‚ùå Exceed GPU memory limits
- ‚ùå Implement features not in the paper

---

## Reference Equations from Paper

### Equation 1: Feature Concatenation
```
f_i = Linear([f_DINO_i | f_FINE_i]) ‚àà R^192
```

### Equations 2-4: Attention Mechanism
```
f_i^T ‚Üê f_i^T + MLP([f_i^T | m_i^(T‚ÜêS)])
m_i^(T‚ÜêS) = Œ£_j Softmax(a_ij^TS)_j * v_j
```

### Equations 5-8: Assignment Matrix
```
P_ij = œÉ_i * œÉ_j * Softmax_k(S_kj)_i * Softmax_k(S_ik)_j
S_ij = Linear(f_i^It)^T * Linear(f_j^It+1)
œÉ_i = Sigmoid(Linear(f_i^It))
```

### Equation 9: Confidence
```
w_ij = ConfMLP([f_i^It | f_j^It+1])
```

### Equations 10-11: Essential Matrix
```
x_j^T * E * x_i = 0
diag(w) * Œ¶ * flat(E) = 0
```

### Equation 12: Matching Loss
```
L_m = -(1/L) Œ£_l [ (1/|M|) Œ£_(i,j)‚ààM log(P_ij) + ... ]
```

### Equation 13: Pose Loss
```
L_p = Œª_t * ||tÃÇ/||tÃÇ|| - t/||t||| + Œª_r * ||Log(RÃÇ) - Log(R)||
```

### Equation 14: Total Loss
```
L_t = (1 - Œª_p) * L_m + Œª_p * L_p
```

---

## Current Implementation Status

### ‚úÖ Phase 1: Environment Setup (COMPLETED)

**Date Completed**: February 5, 2026

#### Completed Tasks:
1. ‚úÖ Created complete project directory structure
2. ‚úÖ Created `requirements.txt` with all dependencies
3. ‚úÖ Installed PyTorch nightly (2.11.0.dev20260205+cu128) for RTX 5060 Ti
4. ‚úÖ Verified GPU functionality (16GB VRAM available)
5. ‚úÖ Downloaded EuRoC Machine Hall dataset (all 5 sequences)
6. ‚úÖ Verified MH_01_easy structure (3,682 images, ground truth, calibration)

#### Important Notes:

**RTX 50 Series GPU Support (CRITICAL)**:
- RTX 5060 Ti uses compute capability sm_120 (Blackwell architecture)
- Stable PyTorch releases (‚â§2.9.x) do NOT support sm_120
- **SOLUTION**: Use PyTorch nightly with CUDA 12.8:
  ```bash
  pip uninstall torch torchvision torchaudio -y
  pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
  ```
- This is a known issue tracked at: https://github.com/pytorch/pytorch/issues/164342
- For new Claude instances: Always check PyTorch version and GPU compatibility first

**Dataset Setup**:
- EuRoC dataset moved to ETH Research Collection
- Old server (robotics.ethz.ch/~asl-datasets) may timeout
- Download from: https://www.research-collection.ethz.ch/handle/20.500.11850/690084
- machine_hall.zip contains nested zips - extract MH_XX_easy.zip files individually
- Dataset location: `data/euroc/MH_01_easy/mav0/`

**Verified Hardware Configuration**:
- GPU: NVIDIA GeForce RTX 5060 Ti (15.93 GB VRAM)
- CUDA: 12.8
- PyTorch: 2.11.0.dev20260205+cu128
- Compute Capability: 12.0 (sm_120)
- Estimated VRAM usage during training: 4-6 GB
- Available headroom: ~10 GB

**Dataset Statistics (MH_01_easy)**:
- Images (cam0): 3,682 frames @ 752√ó480 grayscale
- Images (cam1): 3,682 frames (for stereo extension)
- IMU measurements: 36,820 @ 200Hz (for VIO extension)
- Ground truth poses: 36,382
- Duration: ~184 seconds
- Trajectory length: 0.25 meters (straight-line distance)

#### Scripts Created:
- `scripts/verify_gpu.py` - GPU capability verification
- `scripts/install_pytorch_rtx50.py` - Automated PyTorch installation for RTX 50 series
- `scripts/download_euroc.py` - Dataset downloader with progress bar
- `scripts/verify_dataset.py` - Dataset structure verification
- `scripts/visualize_sample.py` - Quick dataset visualization

---

### üöß Phase 2: Data Pipeline (NEXT)

**Next Steps**:
1. Implement EuRoC dataset loader (`src/datasets/euroc.py`)
   - Load consecutive image pairs
   - Load ground truth poses
   - Load camera intrinsics from sensor.yaml

2. Implement image transforms (`src/datasets/transforms.py`)
   - Resize: 752√ó480 ‚Üí 476√ó742 (as per paper Section IV-A)
   - Normalization: mean/std for DINOv2 input
   - Optional augmentation: brightness, contrast (if needed)

3. Test and visualize
   - Load sample image pairs
   - Verify preprocessing
   - Display overlaid features

---

## Start Command

When ready to begin, say:

"Let's start implementing DINO-VO. Phase 1: Environment Setup. First, I'll create the project directory structure."

Then follow the implementation order step by step.

**For resuming from Phase 2**:

"Phase 1 is complete. Let's proceed with Phase 2: Data Pipeline. First, I'll implement the EuRoC dataset loader."